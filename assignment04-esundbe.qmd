*---
title: Assignment 04
author:
  - name: Emily Sundberg
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-10-08'
date-modified: today
date-format: long
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2

execute:
  echo: true
  eval: true
  output: true
  freeze: auto
---

```{python}
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np

np.random.seed(42)

pio.renderers.default = "notebook+notebook_connected+vscode"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")

# Show Schema and Sample Data
#print("---This is Diagnostic check, No need to print it in the final doc---")

# df.printSchema() # comment this line when rendering the submission
df.show(5)
```

# create eda data frame
```{python}

from pyspark.sql.functions import col, pow
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline

eda_cols = [
  "SALARY",
  "MIN_YEARS_EXPERIENCE",
  "DURATION",
  "COMPANY_IS_STAFFING",
  "IS_INTERNSHIP",
  "STATE_NAME",
  "REMOTE_TYPE_NAME",
  "EMPLOYMENT_TYPE_NAME",
  "MIN_EDULEVELS_NAME"
]

df_eda = df.select(eda_cols)
df_eda.show(5)

```

# Dealing With Missing Data

## Analyzing the Missing Data
### Bar Chart

```{python}
from pyspark.sql.functions import col, sum as spark_sum, when, trim, length
import hvplot.pandas

missing_df = df_eda.select([
  spark_sum(
    when(col(c).isNull() | (length(trim(col(c)))== 0),1).otherwise(0)).alias(c)
    for c in df_eda.columns
])

missing_pd = missing_df.toPandas().T.reset_index()
missing_pd.columns = ["column", "missing_count"]

total_rows = df_eda.count()

missing_pd["missing_percent"]=100*missing_pd["missing_count"]/total_rows

missing_pd.sort_values("missing_percent", ascending=False).hvplot.bar(
  x="column",
  y="missing_percent",
  rot=90,
  title="Percentage of Missing Values by Column",
  height = 600,
  width = 900,
  ylabel = "Missing Percentage (%)",
  xlabel = "Features"
).opts(xrotation=45)

#missing_pd.head()

```

### Heatmap


```{python}
import pandas as pd

df_sample = df_eda.sample(fraction = .05, seed = 42).toPandas()

missing_mask = df_sample.isnull()

missing_long = (
  missing_mask.reset_index()
  .melt(id_vars = "index", var_name = "column", value_name = "is_missing")
)

missing_long["is_missing"] = missing_long["is_missing"].astype(int)

missing_long.hvplot.heatmap(
  x="column",
  y="index",
  C = "is_missing",
  cmap = "Blues",
  width = 900,
  height = 500,
  title = "Heatmap of Missing Values (5%)"
).opts(xrotation=45)

```

### Unique Values

```{python}
from pyspark.sql.functions import countDistinct

df_eda.select([
  countDistinct(c).alias(c+"_nunique")
  for c in df_eda.columns
]).show(truncate=False)



```

### Replacing Null Values and Errors


```{python}

# REMOTE TYPE NAME
df_eda = df_eda.withColumn(
  "REMOTE_TYPE_NAME",
    when(col("REMOTE_TYPE_NAME") == "Remote","Remote")
    .when(col("REMOTE_TYPE_NAME") == "[None]","In-Person")
    .when(col("REMOTE_TYPE_NAME") == "Not Remote","In-Person")
    .when(col("REMOTE_TYPE_NAME") == "Hybrid Remote","Hybrid")
    .when(col("REMOTE_TYPE_NAME").isNull(), "In-Person")
    .otherwise(col("REMOTE_TYPE_NAME"))
)

# EMPLOYMENT TYPE NAME

df_eda = df_eda.withColumn(
  "EMPLOYMENT_TYPE_NAME",
    when(col("EMPLOYMENT_TYPE_NAME") == "Part-time / full-time","Flexible")
    .when(col("EMPLOYMENT_TYPE_NAME") == "Part-time (â‰¤ 32 hours)","Part-Time")
    .when(col("EMPLOYMENT_TYPE_NAME") == "Full-time (> 32 hours)","Full-Time")
    .when(col("EMPLOYMENT_TYPE_NAME").isNull(), "Full-Time")
    .otherwise(col("EMPLOYMENT_TYPE_NAME"))
)

# Minimum Education Levels
df_eda = df_eda.withColumn(
  "MIN_EDULEVELS_NAME",
    when(col("MIN_EDULEVELS_NAME").isNull(), "No Education Listed")
    .otherwise(col("MIN_EDULEVELS_NAME"))
)

# State Name
df_eda = df_eda.withColumn(
  "STATE_NAME",
    when(col("STATE_NAME").isNull(), "Unknown")
    .otherwise(col("STATE_NAME"))
)

# Company Staffing
df_eda = df_eda.withColumn(
  "COMPANY_IS_STAFFING",
    when(col("COMPANY_IS_STAFFING").isNull(), False)
    .otherwise(col("COMPANY_IS_STAFFING"))
)

# Internship
df_eda = df_eda.withColumn(
  "IS_INTERNSHIP",
    when(col("IS_INTERNSHIP").isNull(), False)
    .otherwise(col("IS_INTERNSHIP"))
)

categorical_cols = [
  "STATE_NAME",
  "REMOTE_TYPE_NAME",
  "EMPLOYMENT_TYPE_NAME",
  "MIN_EDULEVELS_NAME",
  "COMPANY_IS_STAFFING",
  "IS_INTERNSHIP"
]

for colname in categorical_cols:
  print(f"\n----{colname} ----")
  df_eda.select(colname).distinct().show(60,truncate = False)



```

# Feature Engineering

```{python}

from pyspark.sql.functions import col, when, expr


med_duration = df_eda.approxQuantile("DURATION",[0.5], 0.01)[0]

df_eda = df_eda.withColumn(
  "DURATION",
  when(col("DURATION").isNull(), med_duration)
)

overall_med_sal = df_eda.approxQuantile("SALARY", [0.5],0.01)[0]

#print(overall_med_sal)

median_sal_employment_type = df_eda.groupBy("EMPLOYMENT_TYPE_NAME").agg(expr("percentile_approx(SALARY,0.5)").alias("median_sal_employment_type"))

df_eda = df_eda.join(median_sal_employment_type, on="EMPLOYMENT_TYPE_NAME", how="left")

df_eda = df_eda.withColumn("SALARY", when(col("SALARY").isNull(),
  when(col("median_sal_employment_type").isNotNull(), col("median_sal_employment_type"))
  .otherwise(overall_med_sal)).otherwise(col("SALARY")))


```

```{python}
df_feature = df_eda.dropna(subset=[
  "SALARY","MIN_YEARS_EXPERIENCE","MIN_EDULEVELS_NAME", "STATE_NAME","EMPLOYMENT_TYPE_NAME","REMOTE_TYPE_NAME","IS_INTERNSHIP","COMPANY_IS_STAFFING", "DURATION"
])

categorical_cols = ["MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME","STATE_NAME"]

indexers = [StringIndexer(inputCol=col, outputCol = f"{col}_idx", handleInvalid = 'skip')for col in categorical_cols]

encoders = [OneHotEncoder(inputCol=f"{col}_idx",outputCol = f"{col}_vec")for col in categorical_cols]

assembler = VectorAssembler(
  inputCols = [
    "MIN_YEARS_EXPERIENCE","DURATION","IS_INTERNSHIP", "COMPANY_IS_STAFFING"
  ] + [f"{col}_vec" for col in categorical_cols],
  outputCol= "features"
)

pipeline = Pipeline(stages = indexers + encoders + [assembler])
data = pipeline.fit(df_feature).transform(df_feature)

data = data.withColumn("MIN_YEARS_EXPERIENCE_SQ", pow(col("MIN_YEARS_EXPERIENCE"),2))




data.select("SALARY","features").show(5, truncate = False)
```

# Train/Test Split


```{python}
regression_train, regression_test = data.randomSplit([0.75,0.25],seed=42)

print((data.count(), len(data.columns)))
print((regression_train.count(), len(regression_train.columns)))
print((regression_test.count(), len(regression_test.columns)))

```

# Linear Regression Model


```{python}
from pyspark.ml.regression import GeneralizedLinearRegression

feature_names = assembler.getInputCols()

glr = GeneralizedLinearRegression(
  featuresCol="features",
  labelCol="SALARY",
  family = "gaussian",
  link = "identity",
  maxIter = 10,
  regParam=0.3
)

glr_model = glr.fit(regression_train)
summary = glr_model.summary

print("\n --- Regression Summary ---")
print("Coefficient Standard Errors:", [f"{val:.4f}" for val in summary.coefficientStandardErrors])
print("T-Values:", [f"{val:.4f}" for val in summary.tValues])
print("P-Values:",[f"{val:.4f}" for val in summary.pValues])


print(f"\nDispersion: {summary.dispersion:.4f}")
print(f"Null Deviance: {summary.nullDeviance:.4f}")
print(f"Residual DF Null: {summary.residualDegreeOfFreedomNull}")
print(f"Deviance: {summary.deviance:.4f}")
print(f"Residual DF: {summary.residualDegreeOfFreedom}")
print(f"AIC: {summary.aic:.4f}")
```

These values tell us that this model is not a good fit.


```{python}
import pandas as pd
from tabulate import tabulate
import pandas as pd 
from IPython.display import HTML

feature_names = summary._call_java("featureNames")
coefs = [glr_model.intercept] +list(glr_model.coefficients)
se = list(summary.coefficientStandardErrors)
tvals = list(summary.tValues)
pvals = list(summary.pValues)

print("features", len(feature_names))
print("coefs", len(coefs))
print("se", len(se))
print("tvals", len(tvals))
print("pvals", len(pvals))

coef_table = pd.DataFrame({
  "Feature": ["Intercept"]+feature_names,
  "Estimate": [f"{v:.4f}" if v is not None else None for v in coefs],
  "Std Error": [f"{v:.4f}" if v is not None else None for v in se],
  "t-stat": [f"{v:.4f}" if v is not None else None for v in tvals],
  "P-Value": [f"{v:.4f}" if v is not None else None for v in pvals],
})

coef_table.to_csv("glr_summary.csv", index=False)


print(coef_table)
``` 








# Poly Assembler
```{python}
assembler_poly = VectorAssembler(
  inputCols=[
    "MIN_YEARS_EXPERIENCE",
    "MIN_YEARS_EXPERIENCE_SQ",
    "DURATION",
    "IS_INTERNSHIP",
    "COMPANY_IS_STAFFING"
  ]+[f"{col}_vec" for col in categorical_cols],
  outputCol = "features_poly"
)

data = assembler_poly.transform(data)

data.select("SALARY","features","features_poly").show(5, truncate = False)
```