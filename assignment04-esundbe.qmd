*---
title: Assignment 04
author:
  - name: Emily Sundberg
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-10-08'
date-modified: today
date-format: long
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2

execute:
  echo: true
  eval: true
  output: true
  freeze: auto
---

```{python}
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np

np.random.seed(42)

pio.renderers.default = "notebook+notebook_connected+vscode"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")

# Show Schema and Sample Data
#print("---This is Diagnostic check, No need to print it in the final doc---")

# df.printSchema() # comment this line when rendering the submission
df.show(5)
```

# create eda data frame
```{python}

from pyspark.sql.functions import col, pow
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline

eda_cols = [
  "SALARY",
  "MIN_YEARS_EXPERIENCE",
  "DURATION",
  "COMPANY_IS_STAFFING",
  "IS_INTERNSHIP",
  "STATE_NAME",
  "REMOTE_TYPE_NAME",
  "EMPLOYMENT_TYPE_NAME",
  "MIN_EDULEVELS_NAME"
]

df_eda = df.select(eda_cols)
df_eda.show(5)

```

# Dealing With Missing Data

## Analyzing the Missing Data
### Bar Chart

```{python}
from pyspark.sql.functions import col, sum as spark_sum, when, trim, length
import hvplot.pandas

missing_df = df_eda.select([
  spark_sum(
    when(col(c).isNull() | (length(trim(col(c)))== 0),1).otherwise(0)).alias(c)
    for c in df_eda.columns
])

missing_pd = missing_df.toPandas().T.reset_index()
missing_pd.columns = ["column", "missing_count"]

total_rows = df_eda.count()

missing_pd["missing_percent"]=100*missing_pd["missing_count"]/total_rows

missing_pd.sort_values("missing_percent", ascending=False).hvplot.bar(
  x="column",
  y="missing_percent",
  rot=90,
  title="Percentage of Missing Values by Column",
  height = 600,
  width = 900,
  ylabel = "Missing Percentage (%)",
  xlabel = "Features"
).opts(xrotation=45)

#missing_pd.head()

```

### Heatmap


```{python}
import pandas as pd

df_sample = df_eda.sample(fraction = .05, seed = 42).toPandas()

missing_mask = df_sample.isnull()

missing_long = (
  missing_mask.reset_index()
  .melt(id_vars = "index", var_name = "column", value_name = "is_missing")
)

missing_long["is_missing"] = missing_long["is_missing"].astype(int)

missing_long.hvplot.heatmap(
  x="column",
  y="index",
  C = "is_missing",
  cmap = "Blues",
  width = 900,
  height = 500,
  title = "Heatmap of Missing Values (5%)"
).opts(xrotation=45)

```

### Unique Values

```{python}
from pyspark.sql.functions import countDistinct

df_eda.select([
  countDistinct(c).alias(c+"_nunique")
  for c in df_eda.columns
]).show(truncate=False)



```

### Replacing Null Values and Errors


```{python}

# REMOTE TYPE NAME
df_eda = df_eda.withColumn(
  "REMOTE_TYPE_NAME",
    when(col("REMOTE_TYPE_NAME") == "Remote","Remote")
    .when(col("REMOTE_TYPE_NAME") == "[None]","In-Person")
    .when(col("REMOTE_TYPE_NAME") == "Not Remote","In-Person")
    .when(col("REMOTE_TYPE_NAME") == "Hybrid Remote","Hybrid")
    .when(col("REMOTE_TYPE_NAME").isNull(), "In-Person")
    .otherwise(col("REMOTE_TYPE_NAME"))
)

# EMPLOYMENT TYPE NAME

df_eda = df_eda.withColumn(
  "EMPLOYMENT_TYPE_NAME",
    when(col("EMPLOYMENT_TYPE_NAME") == "Part-time / full-time","Flexible")
    .when(col("EMPLOYMENT_TYPE_NAME") == "Part-time (â‰¤ 32 hours)","Part-Time")
    .when(col("EMPLOYMENT_TYPE_NAME") == "Full-time (> 32 hours)","Full-Time")
    .when(col("EMPLOYMENT_TYPE_NAME").isNull(), "Full-Time")
    .otherwise(col("EMPLOYMENT_TYPE_NAME"))
)

# Minimum Education Levels
df_eda = df_eda.withColumn(
  "MIN_EDULEVELS_NAME",
    when(col("MIN_EDULEVELS_NAME").isNull(), "No Education Listed")
    .otherwise(col("MIN_EDULEVELS_NAME"))
)

# State Name
df_eda = df_eda.withColumn(
  "STATE_NAME",
    when(col("STATE_NAME").isNull(), "Unknown")
    .otherwise(col("STATE_NAME"))
)

# Company Staffing
df_eda = df_eda.withColumn(
  "COMPANY_IS_STAFFING",
    when(col("COMPANY_IS_STAFFING").isNull(), False)
    .otherwise(col("COMPANY_IS_STAFFING"))
)

# Internship
df_eda = df_eda.withColumn(
  "IS_INTERNSHIP",
    when(col("IS_INTERNSHIP").isNull(), False)
    .otherwise(col("IS_INTERNSHIP"))
)

categorical_cols = [
  "STATE_NAME",
  "REMOTE_TYPE_NAME",
  "EMPLOYMENT_TYPE_NAME",
  "MIN_EDULEVELS_NAME",
  "COMPANY_IS_STAFFING",
  "IS_INTERNSHIP"
]

for colname in categorical_cols:
  print(f"\n----{colname} ----")
  df_eda.select(colname).distinct().show(60,truncate = False)



```

# Feature Engineering

```{python}

from pyspark.sql.functions import col, when, expr


med_duration = df_eda.approxQuantile("DURATION",[0.5], 0.01)[0]

df_eda = df_eda.withColumn(
  "DURATION",
  when(col("DURATION").isNull(), med_duration)
)

overall_med_sal = df_eda.approxQuantile("SALARY", [0.5],0.01)[0]

#print(overall_med_sal)

median_sal_employment_type = df_eda.groupBy("EMPLOYMENT_TYPE_NAME").agg(expr("percentile_approx(SALARY,0.5)").alias("median_sal_employment_type"))

df_eda = df_eda.join(median_sal_employment_type, on="EMPLOYMENT_TYPE_NAME", how="left")

df_eda = df_eda.withColumn("SALARY", when(col("SALARY").isNull(),
  when(col("median_sal_employment_type").isNotNull(), col("median_sal_employment_type"))
  .otherwise(overall_med_sal)).otherwise(col("SALARY")))


```

```{python}
df_feature = df_eda.dropna(subset=[
  "SALARY","MIN_YEARS_EXPERIENCE","MIN_EDULEVELS_NAME", "STATE_NAME","EMPLOYMENT_TYPE_NAME","REMOTE_TYPE_NAME","IS_INTERNSHIP","COMPANY_IS_STAFFING", "DURATION"
])

categorical_cols = ["MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME","STATE_NAME"]

indexers = [StringIndexer(inputCol=col, outputCol = f"{col}_idx", handleInvalid = 'skip')for col in categorical_cols]

encoders = [OneHotEncoder(inputCol=f"{col}_idx",outputCol = f"{col}_vec")for col in categorical_cols]

assembler = VectorAssembler(
  inputCols = [
    "MIN_YEARS_EXPERIENCE","DURATION","IS_INTERNSHIP", "COMPANY_IS_STAFFING"
  ] + [f"{col}_vec" for col in categorical_cols],
  outputCol= "features"
)

pipeline = Pipeline(stages = indexers + encoders + [assembler])
data = pipeline.fit(df_feature).transform(df_feature)

data = data.withColumn("MIN_YEARS_EXPERIENCE_SQ", pow(col("MIN_YEARS_EXPERIENCE"),2))




data.select("SALARY","features").show(5, truncate = False)
```

# Train/Test Split


```{python}
regression_train, regression_test = data.randomSplit([0.75,0.25],seed=42)

print((data.count(), len(data.columns)))
print((regression_train.count(), len(regression_train.columns)))
print((regression_test.count(), len(regression_test.columns)))

```

# Linear Regression Model


```{python}
from pyspark.ml.regression import GeneralizedLinearRegression

feature_names = assembler.getInputCols()

glr = GeneralizedLinearRegression(
  featuresCol="features",
  labelCol="SALARY",
  family = "gaussian",
  link = "identity",
  maxIter = 10,
  regParam=0.3
)

glr_model = glr.fit(regression_train)
summary = glr_model.summary

print("\n --- Regression Summary ---")
print("Coefficient Standard Errors:", [f"{val:.4f}" for val in summary.coefficientStandardErrors])
print("T-Values:", [f"{val:.4f}" for val in summary.tValues])
print("P-Values:",[f"{val:.4f}" for val in summary.pValues])


print(f"\nDispersion: {summary.dispersion:.4f}")
print(f"Null Deviance: {summary.nullDeviance:.4f}")
print(f"Residual DF Null: {summary.residualDegreeOfFreedomNull}")
print(f"Deviance: {summary.deviance:.4f}")
print(f"Residual DF: {summary.residualDegreeOfFreedom}")
print(f"AIC: {summary.aic:.4f}")
```

These values tell us that this model is not a good fit.


```{python}
import pandas as pd
from tabulate import tabulate
import pandas as pd 
from IPython.display import HTML

feature_names = summary._call_java("featureNames")
coefs = [glr_model.intercept] +list(glr_model.coefficients)
se = list(summary.coefficientStandardErrors)
tvals = list(summary.tValues)
pvals = list(summary.pValues)

print("features", len(feature_names))
print("coefs", len(coefs))
print("se", len(se))
print("tvals", len(tvals))
print("pvals", len(pvals))

coef_table = pd.DataFrame({
  "Feature": ["Intercept"]+feature_names,
  "Estimate": [f"{v:.4f}" if v is not None else None for v in coefs],
  "Std Error": [f"{v:.4f}" if v is not None else None for v in se],
  "t-stat": [f"{v:.4f}" if v is not None else None for v in tvals],
  "P-Value": [f"{v:.4f}" if v is not None else None for v in pvals],
})

coef_table.to_csv("glr_summary.csv", index=False)


print(coef_table)
``` 

# Polynomial Regression Model

```{python}

poly_data = data.withColumn("MIN_YEARS_EXPERIENCE_SQ", pow(col("MIN_YEARS_EXPERIENCE"),2))


assembler_poly = VectorAssembler(
  inputCols=[
    "MIN_YEARS_EXPERIENCE",
    "MIN_YEARS_EXPERIENCE_SQ",
    "DURATION",
    "IS_INTERNSHIP",
    "COMPANY_IS_STAFFING"
  ]+[f"{col}_vec" for col in categorical_cols],
  outputCol = "poly_features"
)

poly_data = assembler_poly.transform(poly_data)

poly_data.select("SALARY","features","poly_features").show(5, truncate = False)
```

## Split Data

```{python}
regression_train, regression_test = poly_data.randomSplit([0.75,0.25], seed=42)
print((poly_data.count(), len(poly_data.columns)))
print((regression_train.count(), len(regression_train.columns)))
print((regression_test.count(), len(regression_test.columns)))


```

## regression model
```{python}
from pyspark.ml.regression import GeneralizedLinearRegression

feature_names = assembler.getInputCols()

poly_glr = GeneralizedLinearRegression(
  featuresCol="features",
  labelCol="SALARY",
  family = "gaussian",
  link = "identity",
  maxIter = 10,
  regParam=0.3
)

poly_glr_model = poly_glr.fit(regression_train)
summary = poly_glr_model.summary

print("\n --- Regression Summary ---")
print("Coefficient Standard Errors:", [f"{val:.4f}" for val in summary.coefficientStandardErrors])
print("T-Values:", [f"{val:.4f}" for val in summary.tValues])
print("P-Values:",[f"{val:.4f}" for val in summary.pValues])


print(f"\nDispersion: {summary.dispersion:.4f}")
print(f"Null Deviance: {summary.nullDeviance:.4f}")
print(f"Residual DF Null: {summary.residualDegreeOfFreedomNull}")
print(f"Deviance: {summary.deviance:.4f}")
print(f"Residual DF: {summary.residualDegreeOfFreedom}")
print(f"AIC: {summary.aic:.4f}")
```
```{python}
import pandas as pd
from tabulate import tabulate
import pandas as pd 
from IPython.display import HTML

poly_feature_names = summary._call_java("featureNames")
poly_coefs = [poly_glr_model.intercept] +list(poly_glr_model.coefficients)
poly_se = list(summary.coefficientStandardErrors)
poly_tvals = list(summary.tValues)
poly_pvals = list(summary.pValues)

print("features", len(poly_feature_names))
print("coefs", len(poly_coefs))
print("se", len(poly_se))
print("tvals", len(poly_tvals))
print("pvals", len(poly_pvals))

coef_table = pd.DataFrame({
  "Feature": ["Intercept"]+poly_feature_names,
  "Estimate": [f"{v:.4f}" if v is not None else None for v in coefs],
  "Std Error": [f"{v:.4f}" if v is not None else None for v in se],
  "t-stat": [f"{v:.4f}" if v is not None else None for v in tvals],
  "P-Value": [f"{v:.4f}" if v is not None else None for v in pvals],
})

coef_table.to_csv("poly_glr_summary.csv", index=False)


print(coef_table)
``` 

# Diagnostics
```{python}

#Used AI, prompt and response are in the AI.txt file

# ==========================================================
# Diagnostic Plots for Linear Regression Model
# ==========================================================

import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import pandas as pd

# Generate predictions
predictions = glr_model.transform(regression_test)

# Convert to Pandas
pred_pd = predictions.select("SALARY", "prediction").toPandas()

# Compute residuals
pred_pd["residuals"] = pred_pd["SALARY"] - pred_pd["prediction"]

# Set up 2x2 plot grid
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
fig.suptitle("Linear Regression Diagnostic Plots", fontsize=16, y=1.02)

# 1️⃣ Predicted vs Actual
sns.scatterplot(
    x="SALARY", y="prediction", data=pred_pd,
    ax=axes[0, 0], alpha=0.6, edgecolor=None
)
axes[0, 0].plot(
    [pred_pd["SALARY"].min(), pred_pd["SALARY"].max()],
    [pred_pd["SALARY"].min(), pred_pd["SALARY"].max()],
    color="red", linestyle="--", linewidth=1.2
)
axes[0, 0].set_title("Predicted vs Actual")
axes[0, 0].set_xlabel("Actual Salary")
axes[0, 0].set_ylabel("Predicted Salary")

# 2️⃣ Residuals vs Predicted
sns.scatterplot(
    x="prediction", y="residuals", data=pred_pd,
    ax=axes[0, 1], alpha=0.6, edgecolor=None
)
axes[0, 1].axhline(0, color="red", linestyle="--", linewidth=1.2)
axes[0, 1].set_title("Residuals vs Predicted")
axes[0, 1].set_xlabel("Predicted Salary")
axes[0, 1].set_ylabel("Residuals")

# 3️⃣ Histogram of Residuals
sns.histplot(pred_pd["residuals"], bins=30, kde=True, ax=axes[1, 0], color="skyblue")
axes[1, 0].axvline(0, color="red", linestyle="--", linewidth=1.2)
axes[1, 0].set_title("Histogram of Residuals")
axes[1, 0].set_xlabel("Residual Value")
axes[1, 0].set_ylabel("Frequency")

# 4️⃣ QQ Plot of Residuals
stats.probplot(pred_pd["residuals"], dist="norm", plot=axes[1, 1])
axes[1, 1].set_title("QQ Plot of Residuals")

plt.tight_layout()
plt.show()
```

# Model Evaluation


```{python}
# ==========================================================
# 6. Model Evaluation
# ==========================================================

from pyspark.ml.evaluation import RegressionEvaluator
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# --- Evaluate on test data ---
evaluator_r2 = RegressionEvaluator(
    labelCol="SALARY", predictionCol="prediction", metricName="r2"
)
evaluator_rmse = RegressionEvaluator(
    labelCol="SALARY", predictionCol="prediction", metricName="rmse"
)

r2 = evaluator_r2.evaluate(predictions)
rmse = evaluator_rmse.evaluate(predictions)

print("===============================================")
print("Model Evaluation Metrics")
print(f"R² (Coefficient of Determination): {r2:.4f}")
print(f"RMSE (Root Mean Squared Error):   {rmse:.2f}")
print("===============================================")

# --- Convert predictions to pandas for plotting ---
pred_pd = predictions.select("SALARY", "prediction").toPandas()

# --- Predicted vs Actual Plot ---
plt.figure(figsize=(8, 6))
sns.scatterplot(
    x="SALARY", y="prediction", data=pred_pd,
    alpha=0.6, color="steelblue", edgecolor=None, label="Predicted"
)

# Ideal fit line (y = x)
plt.plot(
    [pred_pd["SALARY"].min(), pred_pd["SALARY"].max()],
    [pred_pd["SALARY"].min(), pred_pd["SALARY"].max()],
    color="red", linestyle="--", linewidth=1.5, label="Ideal Fit (y = x)"
)

plt.title("Model Evaluation: Predicted vs Actual Salary", fontsize=14)
plt.xlabel("Actual Salary")
plt.ylabel("Predicted Salary")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

```
